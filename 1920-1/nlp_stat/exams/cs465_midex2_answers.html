
<!-- saved from url=(0062)http://www.cs.jhu.edu/~hajic/courses/cs465/midex2.answers.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   
   <meta name="Author" content="Jan Hajic">
   <title>NLP / Midterm Exam answers</title>
</head>

<body bgcolor="#ffe8b0" link="#330099" vlink="#006666" alink="#6666cc" text="#000000">
<no body="" bgcolor="#fffcf0" link="#330099" vlink="#006666" alink="#6666cc" text="#000000">

<center>
<h1>600.465 Introduction to NLP (Fall 2000)<br>
</h1>
<h1>Midterm Exam Answers<br>
</h1>
<h1>Date: Oct 30, 2000 2pm (<font color="#f00000">30 min.</font>)</h1>
</center>

<hr width="100%">

<hr width="100%">
<br>&nbsp;

If asked to compute something for which you have the numbers, that
really means to compute the final number, not just to write the
formula. If asked for a formula, write down the formula.

<p>
</p><h2>1. Probability</h2>

Let S = { a, b, c } (the sample space), and p be the joint
distribution on a sequence of two events (i.e. on S x S, ordered).  If
you know that p(a,a) [a followed by a] = 0.25, p(c,c) [c followed by
c] = 0.25, p(b,a) [b followed by a] = 0.125, 
p(b,b) [b followed by b] = 0, p(a,c) [a followed by c]
= 0.25, pL(a) [unigram probability of a as a left-hand bigram member] = .5,
and pR(b) [unigram probability of b as the right-hand bigram 
member] = 0.125, is it enough to compute
p(b|c) (i.e., the probability of seeing b if we already know that the
preceding event generated c)?

<ul><li>Yes / No: __Yes__<br>
<br>
</li><li>why? __The bigram probabilities sum up to 0.875; the unigram constraints
further determine that p(a,b) = 0, thus the remaining
 .125 must be at p(c,b) (from p(a,b) + p(b,b) + p(c,b) = pR(b));
 therefore p is fully defined.___
<br>
<br>
__Therefore we can get p<sub>L</sub>(c) = sum over i of p(c,i), then p(b|c) = p(c,b)/p(c)._
<br>
<br>
</li><li>If yes, compute: p(b|c) = ___1/3__( = p(c,b) / p<sub>L</sub>(c) = .125 / .375)____
</li></ul>

<h2>2. Estimation and Cross-entropy</h2>

Use the bigram distribution p from question 1.

<ul><li>Write one example of a data sequence which faithfully follows the
distribution (i.e., a training data from which we would get the above
bigram distribution using the MLE method):<br>
<br>
<br>
E.g.: __a__&nbsp;&nbsp;
__c__&nbsp;&nbsp;
__c__&nbsp;&nbsp;
__b__&nbsp;&nbsp;
__a__&nbsp;&nbsp;
__a__&nbsp;&nbsp;
__a__&nbsp;&nbsp;
__c__&nbsp;&nbsp;
__c__&nbsp;&nbsp;<br>
<br><br>

</li><li>What is the cross-entropy H<sub>data</sub>(p) in bits and the
perplexity<sup>1</sup> G<sub>data</sub>(p) of the bigram distribution from
question 1 if computed against the following data (use the data-oriented formula for conditional distribution derived from p):<br>
<br>
data = <font size="+1"><tt>b a a a</tt></font><br>
<br>
H<sub>data</sub>(p) = ____5/4______<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;<sub>___</sub><br>
G<sub>data</sub>(p) =  _ &nbsp;2 x <sup>4</sup><font size="+2">V</font><font size="+0">2</font>&nbsp;&nbsp;__ (twice the square root of square root of 2)<br>
<br>


</li></ul>

<h2>3. Mutual information</h2>

Use the bigram distribution from question 1.

<ul>
<li>What is the pointwise mutual information of b and a (in this order)?<br>
<br>I<sub>pointwise</sub>(b,a) = _  3-log<sub>2</sub>(3) = 1.415... _( 
= log<sub>2</sub>(p(b,a)/p<sub>L</sub>(b)p<sub>R</sub>(a)) = 
log<sub>2</sub>(0.125/((0.125)(0.375))) = log<sub>2</sub>(8/3) __<br>
</li></ul><br>

<h2>4. Smoothing and the sparse data problem</h2>

<ul>
<li>Name three methods of smoothing:<br>

<p>
</p><ul><li>____"Add 1" (or "Add lambda")_____________________________<br><br>
</li><li>____Good-Turing___________________________________________<br><br>
</li><li>____Linear Interpolation__________________________________<br><br>
</li></ul>

</li><li>
If you were to design a trigram language model, how would the final smoothed
distribution be defined if you use the linear interpolation smoothing method?
<br>
<br>
<ul>
<li>____p<sub>3</sub>'(w<sub>i</sub>|w<sub>i-2</sub>,w<sub>i-1</sub>) = 
l<sub>3</sub>p<sub>3</sub>(w<sub>i</sub>|w<sub>i-2</sub>,w<sub>i-1</sub>) + 
l<sub>2</sub>p<sub>2</sub>(w<sub>i</sub>|w<sub>i-1</sub>) + 
l<sub>1</sub>p<sub>1</sub>(w<sub>i</sub>) + l<sub>0</sub>(1/|V|), 
l<sub>0</sub> + l<sub>1</sub> + l<sub>2</sub> + l<sub>3</sub> = 1___<br>
</li></ul><br>
</li></ul>

<h2>5. Classes based on Mutual Information</h2>

Suppose you have the following data:<br><br>

<font size="+1"><tt>It is interesting to watch , at least from
the foreign policy perspective , how the wannabe president George W . 
differs from his father , the former president George Bush .
</tt></font><br><br>

What is the best pair of candidates for the first merge, if you use
the greedy algorithm for classes based on bigram mutual information
(i.e. the homework #2 algorithm)? Use your judgment, not computation;
in case of two or more best candidates, write as many as you can
find.<br> <br>

2 solutions:

<ul><li>
Word 1: ___W________________
Word 2: ___Bush_____________
</li></ul><br>

<ul><li>
Word 1: ___wannabe__________
Word 2: ___former___________
</li></ul><br>

<h2>6. Hidden Markov Models</h2>

<ul>
<li>What is the Trellis algorithm good for? (Use max. 5 sentences for
the answer.)<br><br>

____To find the probability of a string based on____________________________
<br><br>
____a parametrized (trained) HMM which presumably_______________
<br><br>
____generated the string._______________________________________________
<br><br>
_________________________________________________________________________
<br><br>
_________________________________________________________________________
<br><br>

</li><li>What is the Viterbi algorithm good for? (Use max. 5 sentences for
the answer.)<br><br>

____To find the best path through a state sequence graph given________________
<br><br>
____a parametrized (trained) HMM and some input data presumably_______________
<br><br>
____generated by that HMM._______________________________________________
<br><br>
_________________________________________________________________________
<br><br>
_________________________________________________________________________
<br><br>


</li></ul>

Now check <u><b>if you have filled in your name and SSN</b></u>. Also,
please carefully check your answers and hand the exam in.

<hr width="100%">

<sup>1</sup> The cross-entropy and perplexity computation is the only one computation
here for which you might need a calculator; it is ok if you use an
expression (use the appropriate (integer) numbers, though!).

<hr width="100%">





</no></body></html>